# -*- coding: utf-8 -*-
"""FinalExtractIDs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bb2Zm3LGKumZyf90_S3U_Z4k05xyTzuy
"""

base_url = "http://bigg.ucsd.edu/models/iEC042_1314/genes/"

import pandas as pd
import requests
from bs4 import BeautifulSoup

gene_data = pd.read_csv('/content/Escherichia coli 042.csv')
gene_data.head()

gene_data = gene_data.fillna('dummy')

for column in gene_data.columns:
  gene_data[column] = gene_data[column].apply(lambda x: x.strip("'"))
gene_data.head()

gene_data = gene_data.drop(['A','B','C','D','E','F'], axis=1)
gene_data.head()

"""http://bigg.ucsd.edu/models/iEC042_1314/genes/{gene_id}"""

for column in gene_data.columns:
  gene_ids = list(gene_data[column])
  gene_ids = list(filter(('dummy').__ne__, gene_ids))

  gene_names = []
  uniprot = []
  interpro = []
  goa = []
  
  #cnt = 0
  for gene_id in gene_ids:
    # cnt += 1
    # if cnt == 5:
    #   break
    full_url = base_url + gene_id
    content = requests.get(full_url)
    soup = BeautifulSoup(content.text, 'html.parser')
    u = []
    i = []
    g = []

    for link in soup.find_all('a'):
      if str(link.get('href')).startswith("http://identifiers"):
        url_parts = str(link.get('href')).split('/')
        if url_parts[-2] == 'uniprot':
          u.append(url_parts[-1])
        elif url_parts[-2] == 'interpro':
          i.append(url_parts[-1])
        elif url_parts[-2] == 'goa':
          g.append(url_parts[-1])

    gene_names.append(gene_id)
    uniprot.append(','.join(u))
    interpro.append(','.join(i))
    goa.append(','.join(g))

  mydf = pd.DataFrame(list(zip(gene_names,uniprot,interpro,goa)), columns = ['gene_id','uniprot','interpro','goa'])
  mydf.to_csv(column+'.csv')

"""# REFER"""

full_url = base_url + gene_ids

content = requests.get(full_url)
content

content.text

soup = BeautifulSoup(content.text, 'html.parser')

soup.find_all('a')

gene_names = []
uniprot = []
interpro = []
goa = []

u = []
i = []
g = []

for link in soup.find_all('a'):
  if str(link.get('href')).startswith("http://identifiers"):
    url_parts = str(link.get('href')).split('/')
    if url_parts[-2] == 'uniprot':
      u.append(url_parts[-1])
    elif url_parts[-2] == 'interpro':
      i.append(url_parts[-1])
    elif url_parts[-2] == 'goa':
      g.append(url_parts[-1])

gene_names.append(gene_ids[0])
uniprot.append(','.join(u))
interpro.append(','.join(i))
goa.append(','.join(g))

print(gene_names,uniprot,interpro,goa)

mydf = pd.DataFrame(list(zip(gene_names,uniprot,interpro,goa)), columns = ['gene_id','uniprot','interpro','goa'])
mydf.head()