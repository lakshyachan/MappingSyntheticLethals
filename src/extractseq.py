# -*- coding: utf-8 -*-
"""ExtractSeq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jxGcdX-JtDgCk8mu3-o4piZRFz-lQ9F

Construct URL for each gene ID and its BiGG page
"""

base_url = "http://bigg.ucsd.edu/models/iSDY_1059/genes/"

"""Import the BeautifulSoup Package"""

import pandas as pd
import requests
from bs4 import BeautifulSoup

"""Import file containing all SLs identified in an organism"""

gene_data = pd.read_csv('/content/Shigella_dysenteriae_Sd197.csv')
gene_data.head()

gene_data = gene_data.fillna('dummy')

"""Remove '' from gene IDs to make it recognisable and usable in URL"""

for column in gene_data.columns:
  gene_data[column] = gene_data[column].apply(lambda x: x.strip("'"))
gene_data.head()

"""Drop A-F if analysis only for SL genes and not reactions"""

gene_data = gene_data.drop(['A','B','C','D','E','F'], axis=1)
gene_data.head()

"""Loop will create a prot.txt file in multi FASTA format and also protein_sequence.csv 
containing all sequences corresponding to gene ID. 
'Skip:' output denotes genes with no sequences on their BiGG URLs."""

#Run this;

dfs = []
f = open("prot.txt","w")

for column in gene_data.columns:
  gene_ids = list(gene_data[column])
  gene_ids = list(filter(('dummy').__ne__, gene_ids))

  gene_names = []
  protein_seq = []
  
  for gene_id in gene_ids:
    #cnt += 1
  
    full_url = base_url + gene_id
    content = requests.get(full_url)
    soup = BeautifulSoup(content.text, 'html.parser')
    try:
      seq = soup.find_all("p", {"class": "sequence"})[1].getText()
      protein_seq.append(seq)
      gene_names.append(gene_id)
      f.write(">")
      f.write(gene_id+'\n')
      f.write(seq+'\n')
    except:
      print("skip: ",gene_id)
  mydf = pd.DataFrame(list(zip(gene_names,protein_seq)), columns = [column+' gene_id',column+' protein'])
  dfs.append(mydf)

f.close()
protein_seq_data = pd.concat(dfs, axis=1)
protein_seq_data.to_csv("protein_sequence.csv",index=False)

"""# REFER (Other Hit & Trials)"""

dfs = []

for column in gene_data.columns:
  gene_ids = list(gene_data[column])
  gene_ids = list(filter(('dummy').__ne__, gene_ids))

  gene_names = []
  protein_seq = []
  
  
  for gene_id in gene_ids:
    

    full_url = base_url + gene_id
    content = requests.get(full_url)
    soup = BeautifulSoup(content.text, 'html.parser')
    try:
      seq = soup.find_all("p", {"class": "sequence"})[1].getText()
      protein_seq.append(seq)
      gene_names.append(gene_id)
    except:
      print("skip: ",gene_id)
  mydf = pd.DataFrame(list(zip(gene_names,protein_seq)), columns = [column+' gene_id',column+' protein'])
  dfs.append(mydf)

protein_seq_data = pd.concat(dfs, axis=1)
protein_seq_data.to_csv("protein_sequence.csv",index=False)

with open('Example2.txt', 'w') as writefile:
    writefile.write("This is line A")

full_url = base_url + gene_ids

content = requests.get(full_url)
content

content.text

soup = BeautifulSoup(content.text, 'html.parser')

soup.find_all('a')

gene_names = []
uniprot = []
interpro = []
goa = []

u = []
i = []
g = []

for link in soup.find_all('a'):
  if str(link.get('href')).startswith("http://identifiers"):
    url_parts = str(link.get('href')).split('/')
    if url_parts[-2] == 'uniprot':
      u.append(url_parts[-1])
    elif url_parts[-2] == 'interpro':
      i.append(url_parts[-1])
    elif url_parts[-2] == 'goa':
      g.append(url_parts[-1])

gene_names.append(gene_ids[0])
uniprot.append(','.join(u))
interpro.append(','.join(i))
goa.append(','.join(g))

print(gene_names,uniprot,interpro,goa)

mydf = pd.DataFrame(list(zip(gene_names,uniprot,interpro,goa)), columns = ['gene_id','uniprot','interpro','goa'])
mydf.head()